# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline 

# Import for statistical methods and distributions
from scipy import stats
from scipy.stats import norm, skew

# Importing various utilities for model selection, preprocessing, and metrics
from sklearn.model_selection import train_test_split, KFold, GroupKFold, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import *  # Import all metrics (e.g., accuracy, precision, etc.)

# For system-level and random operations
import sys, os
import random 

# If no warnings options are set, suppress all warnings
if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

# For IPython display and utilities
from IPython import display, utils

# Load the datasets and process time-related columns
solar_wind = pd.read_csv("solar_wind.csv")
solar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)
solar_wind.set_index(["period", "timedelta"], inplace=True)

dst = pd.read_csv("labels.csv")
dst.timedelta = pd.to_timedelta(dst.timedelta)
dst.set_index(["period", "timedelta"], inplace=True)

sunspots = pd.read_csv("sunspots.csv")
sunspots.timedelta = pd.to_timedelta(sunspots.timedelta)
sunspots.set_index(["period", "timedelta"], inplace=True)

# Group data by period and describe it to understand its statistics
dst.groupby("period").describe()

# Preview of the dataset
dst

# Printing shapes and first rows for solar wind and sunspot data
print("Solar wind shape: ", solar_wind.shape)
solar_wind.head()

print("Sunspot shape: ", sunspots.shape)
sunspots.head()

# Group and describe the solar wind data by period for further analysis
solar_wind.groupby("period").describe().T

# Group and describe sunspots data by period
sunspots.groupby("period").describe().T

# Use the 'fivethirtyeight' style for plots
plt.style.use('fivethirtyeight')

# Define function for visualizing different features in the data
def show_raw_visualization(data):
    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), dpi=80)
    for i, key in enumerate(data.columns):
        t_data = data[key]
        ax = t_data.plot(
            ax=axes[i // 2, i % 2],  # Specify the subplot location
            title=f"{key.capitalize()}",  # Set the title for each subplot
            rot=25, color='teal', lw=1.2  # Set rotation, color, and line width
        )

    fig.subplots_adjust(hspace=0.8)  # Adjust spacing between plots
    plt.tight_layout()  # Make the layout neat

# Define the columns to plot and call the visualization function
cols_to_plot = ["bx_gse", "bx_gsm", "bt", "density", "speed", "temperature"]
show_raw_visualization(solar_wind[cols_to_plot].iloc[:1000])  # Plot first 1000 rows

# Check for missing values in the solar wind dataset
solar_wind.isnull().sum()

# Join datasets and fill missing values forward
joined = solar_wind.join(sunspots).join(dst).fillna(method="ffill")

# Plot a heatmap of the correlations between features in the dataset
plt.figure(figsize=(20, 15))
sns.clustermap(joined.corr(), annot=True)

# Set random seeds for reproducibility
from numpy.random import seed
from tensorflow.random import set_seed

seed(2020)
set_seed(2021)

# Import the StandardScaler for feature scaling
from sklearn.preprocessing import StandardScaler

# Solar wind features selected for modeling
SOLAR_WIND_FEATURES = [
    "bt", "temperature", "bx_gse", "by_gse", "bz_gse", "speed", "density",
]

# Define feature columns for modeling
XCOLS = (
    [col + "_mean" for col in SOLAR_WIND_FEATURES]  # Mean of the features
    + [col + "_std" for col in SOLAR_WIND_FEATURES]  # Standard deviation of features
    + ["smoothed_ssn"]  # Smoothed sunspot number
)

# Function to impute missing values in the dataset
def impute_features(feature_df):
    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method="ffill")  # Fill missing sunspot data forward
    feature_df = feature_df.interpolate()  # Interpolate missing solar wind values
    return feature_df

# Function to aggregate hourly data by mean and std
def aggregate_hourly(feature_df, aggs=["mean", "std"]):
    agged = feature_df.groupby(
        ["period", feature_df.index.get_level_values(1).floor("H")]
    ).agg(aggs)  # Group by period and hour, then aggregate
    agged.columns = ["_".join(x) for x in agged.columns]  # Flatten hierarchical column names
    return agged

# Function to preprocess features
def preprocess_features(solar_wind, sunspots, scaler=None, subset=None):
    if subset:
        solar_wind = solar_wind[subset]

    # Aggregate data and join sunspot data
    hourly_features = aggregate_hourly(solar_wind).join(sunspots)

    # Scale the data
    if scaler is None:
        scaler = StandardScaler()  # Use StandardScaler by default
        scaler.fit(hourly_features)

    normalized = pd.DataFrame(
        scaler.transform(hourly_features),
        index=hourly_features.index,
        columns=hourly_features.columns,
    )

    # Impute missing values after scaling
    imputed = impute_features(normalized)

    return imputed, scaler

# Preprocess the features using the specified solar wind features
features, scaler = preprocess_features(solar_wind, sunspots, subset=SOLAR_WIND_FEATURES)
print(features.shape)
features.head()  # Preview the processed features

# Ensure that there are no missing values in the processed features
assert (features.isna().sum() == 0).all()

# Define label columns (t0 and t1)
YCOLS = ["t0", "t1"]

# Function to process the labels (dst)
def process_labels(dst):
    y = dst.copy()  # Copy the dataset
    y["t1"] = y.groupby("period").dst.shift(-1)  # Shift the 'dst' values by 1
    y.columns = YCOLS  # Rename the columns
    return y

# Process the labels
labels = process_labels(dst)
labels.head()

# Join the features and labels to create the final dataset
data = labels.join(features)
data.head()

# Function to split the dataset into train, test, and validation sets
def get_train_test_val(data, test_per_period, val_per_period):
    test = data.groupby("period").tail(test_per_period)  # Select the last 'test_per_period' entries for test
    interim = data[~data.index.isin(test.index)]  # Exclude test data from interim
    val = data.groupby("period").tail(val_per_period)  # Select the last 'val_per_period' entries for validation
    train = interim[~interim.index.isin(val.index)]  # Remaining data becomes the training set
    return train, test, val

# Get the train, test, and validation sets
train, test, val = get_train_test_val(data, test_per_period=6_000, val_per_period=3_000)

# Define the model architecture using TensorFlow and Keras
import tensorflow as tf
from keras import preprocessing

# Configuration for data
data_config = {
    "timesteps": 32,  # Number of time steps in the sequence
    "batch_size": 32,  # Batch size for training
}

# Function to create a timeseries dataset from the DataFrame
def timeseries_dataset_from_df(df, batch_size):
    dataset = None
    timesteps = data_config["timesteps"]

    # Iterate through periods in the dataset
    for _, period_df in df.groupby("period"):
        # Align features and labels so that first 32 inputs are aligned with the 33rd target
        inputs = period_df[XCOLS][:-timesteps]
        outputs = period_df[YCOLS][timesteps:]

        # Create a timeseries dataset
        period_ds = tf.keras.preprocessing.timeseries_dataset_from_array(
            inputs,
            outputs,
            timesteps,
            batch_size=batch_size,
        )

        if dataset is None:
            dataset = period_ds
        else:
            dataset = dataset.concatenate(period_ds)

    return dataset

# Create training and validation datasets
train_ds = timeseries_dataset_from_df(train, data_config["batch_size"])
val_ds = timeseries_dataset_from_df(val, data_config["batch_size"])

# Print the number of train and validation batches
print(f"Number of train batches: {len(train_ds)}")
print(f"Number of val batches: {len(val_ds)}")

# Define the LSTM model architecture
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense, LSTM

# Define model configurations
model_config = {"n_epochs": 20, "n_neurons": 512, "dropout": 0.4, "stateful": False}

# Define the sequential model
model = Sequential()
model.add(
    LSTM(
        model_config["n_neurons"],  # Number of neurons in the LSTM layer
        batch_input_shape=(None, data_config["timesteps"], len(XCOLS)),  # Input shape
        stateful=model_config["stateful"],  # Whether the LSTM is stateful
        dropout=model_config["dropout"],  # Dropout rate
    )
)
model.add(Dense(len(YCOLS)))  # Output layer size based on number of labels
model.compile(
    loss="mean_squared_error",  # Loss function
    optimizer="adam",  # Optimizer
)

model.summary()  # Print model summary

# Train the model
history = model.fit(
    train_ds,
    batch_size=data_config["batch_size"],
    epochs=model_config["n_epochs"],
    verbose=1,
    shuffle=False,
    validation_data=val_ds,
)

# Plot the training history (loss curves)
for name, values in history.history.items():
    plt.plot(values)

# Evaluate the model on the test set
test_ds = timeseries_dataset_from_df(test, data_config["batch_size"])
mse = model.evaluate(test_ds)
print(f"Test RMSE: {mse**.5:.2f}")

# Save the model and the scaler
import json
import pickle

model.save("model")  # Save the trained model

# Save the scaler (used for preprocessing)
with open("scaler.pck", "wb") as f:
    pickle.dump(scaler, f)

# Save the configuration file
data_config["solar_wind_subset"] = SOLAR_WIND_FEATURES  # Add the feature set used to the config
print(data_config)
with open("config.json", "w") as f:
    json.dump(data_config, f)  # Save config to a JSON file

# Example of calculating precision, recall, and F1 score for a binary classification task
from sklearn.metrics import precision_score

# Define actual and predicted values for testing precision
act_pos = [1 for _ in range(100)]
act_neg = [0 for _ in range(10000)]
y_true = act_pos + act_neg
pred_pos = [0 for _ in range(10)] + [1 for _ in range(90)]
pred_neg = [1 for _ in range(30)] + [0 for _ in range(9970)]
y_pred = pred_pos + pred_neg

# Calculate and print precision
precision = precision_score(y_true, y_pred, average='binary')
print('Precision: %.3f' % precision)

# Calculate recall for the dataset with 90 true positives and 10 false negatives
from sklearn.metrics import recall_score
recall = recall_score(y_true, y_pred, average='binary')
print('Recall: %.3f' % recall)

# Calculate F1 score for the dataset with 95 true positives and 55 false positives
from sklearn.metrics import f1_score
score = f1_score(y_true, y_pred, average='binary')
print('F-Measure: %.3f' % score)

# Example of calculating accuracy score for a classification task
from sklearn.metrics import accuracy_score
y_pred = [0, 5, 2, 4]
y_true = [0, 1, 2, 3]
accuracy_score(y_true, y_pred)  # Print the accuracy score
